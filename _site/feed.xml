<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-05-25T03:23:22+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">birdfoot</title><subtitle>my study, project blog</subtitle><author><name>birdfoot</name></author><entry><title type="html">K-nearest-neighbors-regression</title><link href="http://localhost:4000/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0/2024/05/24/K-nearest-neighbors-regression.html" rel="alternate" type="text/html" title="K-nearest-neighbors-regression" /><published>2024-05-24T00:00:00+09:00</published><updated>2024-05-24T00:00:00+09:00</updated><id>http://localhost:4000/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0/2024/05/24/K-nearest-neighbors-regression</id><content type="html" xml:base="http://localhost:4000/%ED%98%BC%EA%B3%B5%EB%A8%B8%EC%8B%A0/2024/05/24/K-nearest-neighbors-regression.html"><![CDATA[<h2 id="지도-학습-알고리즘">지도 학습 알고리즘</h2>

<p><strong>분류</strong> : 샘플을 몇 개의 클래스 중 하나로 분류하는 문제<br />
<strong>회귀</strong> : 정해진 클래스가 아닌 임의의 수치를 예측하는 문제</p>

<h2 id="k-최근접-이웃">k-최근접 이웃</h2>

<p><strong>분류</strong> : 가까운 k개 중에 제일 많은 클래스가 샘플의 클래스가 된다<br />
<strong>회귀</strong> : 가까운 k개의 평균이 샘플의 수치가 된다</p>

<h2 id="회귀의-탄생">‘회귀’의 탄생</h2>

<p><strong>WHO?</strong> 19세기 통계학자이자 사회학자인 프랜시스 골턴<br />
<strong>HOW?</strong> 그는 키가 큰 사람의 아이가 부모보다 더 크지 않다는 사실을 관찰하고 이를 ‘평균으로 회귀한다’라고 표현했다. 그 후 두 변수 사이의 상관관계를 분석하는 방법을 회귀라고 불렀다.</p>

<h2 id="k-최근접-이웃-회귀-모델">k-최근접 이웃 회귀 모델</h2>

<p><strong>데이터준비</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">perch_length</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">8.4</span><span class="p">,</span> <span class="mf">13.7</span><span class="p">,</span> <span class="mf">15.0</span><span class="p">,</span> <span class="mf">16.2</span><span class="p">,</span> <span class="mf">17.4</span><span class="p">,</span> <span class="mf">18.0</span><span class="p">,</span> <span class="mf">18.7</span><span class="p">,</span> <span class="mf">19.0</span><span class="p">,</span>
       <span class="mf">19.6</span><span class="p">,</span> <span class="mf">20.0</span><span class="p">,</span> <span class="mf">21.0</span><span class="p">,</span> <span class="mf">21.0</span><span class="p">,</span> <span class="mf">21.0</span><span class="p">,</span> <span class="mf">21.3</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">,</span> <span class="mf">22.0</span><span class="p">,</span>
       <span class="mf">22.5</span><span class="p">,</span> <span class="mf">22.5</span><span class="p">,</span> <span class="mf">22.7</span><span class="p">,</span> <span class="mf">23.0</span><span class="p">,</span> <span class="mf">23.5</span><span class="p">,</span> <span class="mf">24.0</span><span class="p">,</span> <span class="mf">24.0</span><span class="p">,</span> <span class="mf">24.6</span><span class="p">,</span> <span class="mf">25.0</span><span class="p">,</span> <span class="mf">25.6</span><span class="p">,</span> <span class="mf">26.5</span><span class="p">,</span>
       <span class="mf">27.3</span><span class="p">,</span> <span class="mf">27.5</span><span class="p">,</span> <span class="mf">27.5</span><span class="p">,</span> <span class="mf">27.5</span><span class="p">,</span> <span class="mf">28.0</span><span class="p">,</span> <span class="mf">28.7</span><span class="p">,</span> <span class="mf">30.0</span><span class="p">,</span> <span class="mf">32.8</span><span class="p">,</span> <span class="mf">34.5</span><span class="p">,</span> <span class="mf">35.0</span><span class="p">,</span> <span class="mf">36.5</span><span class="p">,</span>
       <span class="mf">36.0</span><span class="p">,</span> <span class="mf">37.0</span><span class="p">,</span> <span class="mf">37.0</span><span class="p">,</span> <span class="mf">39.0</span><span class="p">,</span> <span class="mf">39.0</span><span class="p">,</span> <span class="mf">39.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">42.0</span><span class="p">,</span>
       <span class="mf">43.0</span><span class="p">,</span> <span class="mf">43.0</span><span class="p">,</span> <span class="mf">43.5</span><span class="p">,</span> <span class="mf">44.0</span><span class="p">])</span>
<span class="n">perch_weight</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.9</span><span class="p">,</span> <span class="mf">32.0</span><span class="p">,</span> <span class="mf">40.0</span><span class="p">,</span> <span class="mf">51.5</span><span class="p">,</span> <span class="mf">70.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">,</span> <span class="mf">78.0</span><span class="p">,</span> <span class="mf">80.0</span><span class="p">,</span>
       <span class="mf">85.0</span><span class="p">,</span> <span class="mf">85.0</span><span class="p">,</span> <span class="mf">110.0</span><span class="p">,</span> <span class="mf">115.0</span><span class="p">,</span> <span class="mf">125.0</span><span class="p">,</span> <span class="mf">130.0</span><span class="p">,</span> <span class="mf">120.0</span><span class="p">,</span> <span class="mf">120.0</span><span class="p">,</span> <span class="mf">130.0</span><span class="p">,</span>
       <span class="mf">135.0</span><span class="p">,</span> <span class="mf">110.0</span><span class="p">,</span> <span class="mf">130.0</span><span class="p">,</span> <span class="mf">150.0</span><span class="p">,</span> <span class="mf">145.0</span><span class="p">,</span> <span class="mf">150.0</span><span class="p">,</span> <span class="mf">170.0</span><span class="p">,</span> <span class="mf">225.0</span><span class="p">,</span> <span class="mf">145.0</span><span class="p">,</span>
       <span class="mf">188.0</span><span class="p">,</span> <span class="mf">180.0</span><span class="p">,</span> <span class="mf">197.0</span><span class="p">,</span> <span class="mf">218.0</span><span class="p">,</span> <span class="mf">300.0</span><span class="p">,</span> <span class="mf">260.0</span><span class="p">,</span> <span class="mf">265.0</span><span class="p">,</span> <span class="mf">250.0</span><span class="p">,</span> <span class="mf">250.0</span><span class="p">,</span>
       <span class="mf">300.0</span><span class="p">,</span> <span class="mf">320.0</span><span class="p">,</span> <span class="mf">514.0</span><span class="p">,</span> <span class="mf">556.0</span><span class="p">,</span> <span class="mf">840.0</span><span class="p">,</span> <span class="mf">685.0</span><span class="p">,</span> <span class="mf">700.0</span><span class="p">,</span> <span class="mf">700.0</span><span class="p">,</span> <span class="mf">690.0</span><span class="p">,</span>
       <span class="mf">900.0</span><span class="p">,</span> <span class="mf">650.0</span><span class="p">,</span> <span class="mf">820.0</span><span class="p">,</span> <span class="mf">850.0</span><span class="p">,</span> <span class="mf">900.0</span><span class="p">,</span> <span class="mf">1015.0</span><span class="p">,</span> <span class="mf">820.0</span><span class="p">,</span> <span class="mf">1100.0</span><span class="p">,</span> <span class="mf">1000.0</span><span class="p">,</span>
       <span class="mf">1100.0</span><span class="p">,</span> <span class="mf">1000.0</span><span class="p">,</span> <span class="mf">1000.0</span><span class="p">])</span>
</code></pre></div></div>

<p><strong>데이터의 형태 파악을 위한 시각화 (x축: 특성 데이터인 길이, y축: 타깃 데이터인 무게)</strong></p>

<p>농어 길이와 무게가 비례하는 것을 확인할 수 있다</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">perch_length</span><span class="p">,</span> <span class="n">perch_weight</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'length'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'weight'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>결과 » <img src="https://velog.velcdn.com/images/koeunbi093/post/6c52f094-cae1-40c9-b657-e08d46442b92/image.png" alt="" /></p>

<hr />

<p><strong>데이터를 모델에 사용하기 전에 테스트와 훈련 세트 나누기</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">train_input</span><span class="p">,</span> <span class="n">test_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span>
<span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">perch_length</span><span class="p">,</span> <span class="n">perch_weight</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div></div>

<p>perch_length가 1차원 배열이기 때문에 이를 나눈 train_input과 test_input도 1차원이다</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">train_input</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<p>결과 » (42,)</p>

<hr />

<p>사이킷런에서 훈련을 하기 위해 reshape 함수를 사용해서 2차원으로 바꾸기<br />
ex) [1,2,3] -&gt; [ [1],[2],[3] ] , 크기 (3, ) -&gt; 크기(3, 1)</p>

<p>reshape() 함수는 지정한 크기가 원소 개수와 다르면 에러가 발생한다</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_input</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>

<p>결과 » <span style="color: red">cannot reshape array of size 42 into shape (2,3)</span></p>

<hr />

<p>크기에 -1을 지정하면 나머지 원소 개수로 모두 채워준다</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># train_input.reshape(42,1)도 가능
</span><span class="n">train_input</span> <span class="o">=</span> <span class="n">train_input</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">test_input</span><span class="o">=</span><span class="n">test_input</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">train_input</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<p>결과 » (42, 1)</p>

<hr />

<p>객체 생성과 회귀 모델 훈련하기</p>

<p>k-최근접 이웃 분류에서 사용한 클래스 KNeighborsClassfier과 비슷한 KNeighborsRegressor을 k-최근접 이웃 회귀에서 사용한다</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>
<span class="n">knr</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">()</span>
<span class="n">knr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
</code></pre></div></div>

<p>테스트 세트의 테스트 점수 확인</p>

<p>분류: 정확도 (샘플을 정확하게 분류한 개수의 비율)<br />
회귀: 결정계수 (정확하게 맞히는 것은 불가능하기에 정확도가 아닌 결정계수)</p>

<p>결정계수 = 1- (타깃-예측)^2의 합/ (타깃- 평균)^2의 합</p>

<p>분자와 분모가 비슷해지면(예측이 타깃의 평균가 비슷해지면) 결정계수는 0에 가까워지고
타깃과 예측이 비슷해지면 결정계수는 1에 가까운 값이 된다</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">knr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_input</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>

<p>결과 » 0.992809406101064</p>

<hr />

<p>타깃과 예측의 절댓값 오차 평균</p>

<p>이 외에도 타깃과 예측한 값 사이를 구해서 예측이 어느정도 벗어났는지 확인할 수 있다</p>

<p>예측값이 평균적으로 19g 정도 타깃값과 다르다는 것을 알 수 있다</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>
<span class="n">test_prediction</span> <span class="o">=</span> <span class="n">knr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>
<span class="n">mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">test_target</span><span class="p">,</span> <span class="n">test_prediction</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">mae</span><span class="p">)</span>
</code></pre></div></div>

<p>결과 » 19.157142857142862</p>

<hr />

<p>훈련 세트의 테스트 점수 확인</p>

<p>훈련세트로 훈련을 했기에 훈련 점수가 더 높아야 함에도 불구하고 테스트 세트의 점수가 더 높다.<br />
이는 과소적합 됐다고 볼 수 있다</p>

<p>과대적합: 훈련세트 점수는 좋지만 테스트 세트의 점수가 굉장히 안 좋다<br />
과소적합: 테스트 세트의 점수가 더 좋거나 두 점수 모두 너무 낮다</p>

<p>과소 적합은 훈련, 테스트 세트의 크기가 매우 작거나 모델이 단순해서 훈련이 제대로 되지 않았을 때 일어난다</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">knr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span><span class="n">train_target</span><span class="p">))</span>
</code></pre></div></div>

<p>결과 »0.9698823289099254</p>

<hr />

<p>과소 적합을 해결하기 위해 모델을 조금 더 복잡하게 만들겠다</p>

<p>k-최근접 이웃 알고리즘 모델은 k의 개수를 줄이면 더 복잡하게 만들 수 있다<br />
이웃의 개수를 줄이면 훈련 세트에 잇는 국지적인 패턴에 민감해지고 이웃의 개수를 늘리면 데이터 전반의 일반적인 패턴을 따를 것이다</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">knr</span><span class="p">.</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span>

<span class="n">knr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
</code></pre></div></div>

<p>테스트와 훈련 세트의 점수 확인</p>

<p>테스트 세트의 점수가 더 낮아졌으니 과소적합 문제가 해결됐다</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">knr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">knr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_input</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>

<p>결과 » 0.9804899950518966
0.9746459963987609</p>]]></content><author><name>birdfoot</name></author><category term="혼공머신" /><summary type="html"><![CDATA[지도 학습 알고리즘]]></summary></entry></feed>